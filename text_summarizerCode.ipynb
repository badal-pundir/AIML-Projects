{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOyEYQiJV07zT3fwiqQyvwq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/badal-pundir/AIML-Projects/blob/main/text_summarizerCode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuMU75Hx6aGb",
        "outputId": "5e1e3c18-1d13-47f0-e4f2-856ae87e93a3",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (70.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (0.43.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U pip setuptools wheel --root-user-action=ignore\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy --root-user-action=ignore\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jS2LGin6k7K",
        "outputId": "4c9d3bae-15cf-4320-b481-df503f0f698e",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.7.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (70.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.6.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm\n"
      ],
      "metadata": {
        "id": "1bIsKxyP6p77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "b77a06cf-174c-4b0e-b6ab-7d3c89308b23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.7.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (70.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.6.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "600d6dyT65IA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from string import punctuation"
      ],
      "metadata": {
        "id": "aODSr4Ze8G9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = '''text summarization techniques\n",
        "Text Summarization Techniques Overview\n",
        "Text summarization is a crucial task in natural language processing (NLP) that involves condensing a large amount of text into a shorter form while preserving the essential information. There are various approaches to text summarization, which can be broadly categorized into extractive and abstractive methods.\n",
        "Extractive Summarization\n",
        "Extractive summarization involves selecting the most relevant sentences or phrases from the original text to form a summary. This technique is based on the idea that the most important information is contained in the sentences that are most frequently mentioned or have the highest frequency of keywords.\n",
        "Examples of extractive summarization techniques include:\n",
        "TextRank: This algorithm assigns scores to each sentence based on the frequency of keywords and selects the top-ranked sentences for the summary.\n",
        "LSA (Latent Semantic Analysis): This technique uses vector space modeling to identify the most relevant sentences and phrases.\n",
        "Luhn: This algorithm identifies the most important sentences based on the frequency of keywords and sentence length.\n",
        "Abstractive Summarization\n",
        "Abstractive summarization involves generating a new summary text by paraphrasing the original content. This technique requires more advanced natural language processing (NLP) techniques and can produce more concise and fluent summaries.\n",
        "Examples of abstractive summarization techniques include:\n",
        "T5 Transformers: This model uses a transformer architecture to generate a summary text.\n",
        "BART Transformers: This model uses a bidirectional encoder representation from transformers (BERT) to generate a summary text.\n",
        "GPT-2 Transformers: This model uses a transformer architecture to generate a summary text.\n",
        "XLM Transformers: This model uses a cross-lingual language model to generate a summary text.\n",
        "Implementation in Python\n",
        "Python has several libraries and tools that can be used for text summarization, including:\n",
        "Gensim: This library provides a range of methods for text preprocessing, topic modeling, and document similarity analysis.\n",
        "Sumy: This library provides a range of methods for text summarization, including extractive and abstractive techniques.\n",
        "NLTK (Natural Language Toolkit): This library provides a range of tools and resources for natural language processing tasks, including text summarization.\n",
        "Conclusion\n",
        "Text summarization is a critical task in NLP that involves condensing a large amount of text into a shorter form while preserving the essential information. There are various approaches to text summarization, including extractive and abstractive methods. Python has several libraries and tools that can be used for text summarization, including Gensim, Sumy, and NLTK.'''"
      ],
      "metadata": {
        "id": "O-GHDvLW8Qeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = list(STOP_WORDS)"
      ],
      "metadata": {
        "id": "vubyKGQ48tvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X635f03480rE",
        "outputId": "fd47b883-f877-4afb-e8fa-066366ac8cbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "C9ARh8GY83xO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(text)"
      ],
      "metadata": {
        "id": "Fg7EZej-9HON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [token.text for token in doc]\n",
        "# This accesses the text of each token. The Token object has an attribute .text\n",
        "# which contains the actual string representation of the token."
      ],
      "metadata": {
        "id": "FvgsWmqQ9LSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrO51HJ69bwc",
        "outputId": "247e9325-fc14-47a3-c1ae-d7aa99138394"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['text',\n",
              " 'summarization',\n",
              " 'techniques',\n",
              " '\\n',\n",
              " 'Text',\n",
              " 'Summarization',\n",
              " 'Techniques',\n",
              " 'Overview',\n",
              " '\\n',\n",
              " 'Text',\n",
              " 'summarization',\n",
              " 'is',\n",
              " 'a',\n",
              " 'crucial',\n",
              " 'task',\n",
              " 'in',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " '(',\n",
              " 'NLP',\n",
              " ')',\n",
              " 'that',\n",
              " 'involves',\n",
              " 'condensing',\n",
              " 'a',\n",
              " 'large',\n",
              " 'amount',\n",
              " 'of',\n",
              " 'text',\n",
              " 'into',\n",
              " 'a',\n",
              " 'shorter',\n",
              " 'form',\n",
              " 'while',\n",
              " 'preserving',\n",
              " 'the',\n",
              " 'essential',\n",
              " 'information',\n",
              " '.',\n",
              " 'There',\n",
              " 'are',\n",
              " 'various',\n",
              " 'approaches',\n",
              " 'to',\n",
              " 'text',\n",
              " 'summarization',\n",
              " ',',\n",
              " 'which',\n",
              " 'can',\n",
              " 'be',\n",
              " 'broadly',\n",
              " 'categorized',\n",
              " 'into',\n",
              " 'extractive',\n",
              " 'and',\n",
              " 'abstractive',\n",
              " 'methods',\n",
              " '.',\n",
              " '\\n',\n",
              " 'Extractive',\n",
              " 'Summarization',\n",
              " '\\n',\n",
              " 'Extractive',\n",
              " 'summarization',\n",
              " 'involves',\n",
              " 'selecting',\n",
              " 'the',\n",
              " 'most',\n",
              " 'relevant',\n",
              " 'sentences',\n",
              " 'or',\n",
              " 'phrases',\n",
              " 'from',\n",
              " 'the',\n",
              " 'original',\n",
              " 'text',\n",
              " 'to',\n",
              " 'form',\n",
              " 'a',\n",
              " 'summary',\n",
              " '.',\n",
              " 'This',\n",
              " 'technique',\n",
              " 'is',\n",
              " 'based',\n",
              " 'on',\n",
              " 'the',\n",
              " 'idea',\n",
              " 'that',\n",
              " 'the',\n",
              " 'most',\n",
              " 'important',\n",
              " 'information',\n",
              " 'is',\n",
              " 'contained',\n",
              " 'in',\n",
              " 'the',\n",
              " 'sentences',\n",
              " 'that',\n",
              " 'are',\n",
              " 'most',\n",
              " 'frequently',\n",
              " 'mentioned',\n",
              " 'or',\n",
              " 'have',\n",
              " 'the',\n",
              " 'highest',\n",
              " 'frequency',\n",
              " 'of',\n",
              " 'keywords',\n",
              " '.',\n",
              " '\\n',\n",
              " 'Examples',\n",
              " 'of',\n",
              " 'extractive',\n",
              " 'summarization',\n",
              " 'techniques',\n",
              " 'include',\n",
              " ':',\n",
              " '\\n',\n",
              " 'TextRank',\n",
              " ':',\n",
              " 'This',\n",
              " 'algorithm',\n",
              " 'assigns',\n",
              " 'scores',\n",
              " 'to',\n",
              " 'each',\n",
              " 'sentence',\n",
              " 'based',\n",
              " 'on',\n",
              " 'the',\n",
              " 'frequency',\n",
              " 'of',\n",
              " 'keywords',\n",
              " 'and',\n",
              " 'selects',\n",
              " 'the',\n",
              " 'top',\n",
              " '-',\n",
              " 'ranked',\n",
              " 'sentences',\n",
              " 'for',\n",
              " 'the',\n",
              " 'summary',\n",
              " '.',\n",
              " '\\n',\n",
              " 'LSA',\n",
              " '(',\n",
              " 'Latent',\n",
              " 'Semantic',\n",
              " 'Analysis',\n",
              " '):',\n",
              " 'This',\n",
              " 'technique',\n",
              " 'uses',\n",
              " 'vector',\n",
              " 'space',\n",
              " 'modeling',\n",
              " 'to',\n",
              " 'identify',\n",
              " 'the',\n",
              " 'most',\n",
              " 'relevant',\n",
              " 'sentences',\n",
              " 'and',\n",
              " 'phrases',\n",
              " '.',\n",
              " '\\n',\n",
              " 'Luhn',\n",
              " ':',\n",
              " 'This',\n",
              " 'algorithm',\n",
              " 'identifies',\n",
              " 'the',\n",
              " 'most',\n",
              " 'important',\n",
              " 'sentences',\n",
              " 'based',\n",
              " 'on',\n",
              " 'the',\n",
              " 'frequency',\n",
              " 'of',\n",
              " 'keywords',\n",
              " 'and',\n",
              " 'sentence',\n",
              " 'length',\n",
              " '.',\n",
              " '\\n',\n",
              " 'Abstractive',\n",
              " 'Summarization',\n",
              " '\\n',\n",
              " 'Abstractive',\n",
              " 'summarization',\n",
              " 'involves',\n",
              " 'generating',\n",
              " 'a',\n",
              " 'new',\n",
              " 'summary',\n",
              " 'text',\n",
              " 'by',\n",
              " 'paraphrasing',\n",
              " 'the',\n",
              " 'original',\n",
              " 'content',\n",
              " '.',\n",
              " 'This',\n",
              " 'technique',\n",
              " 'requires',\n",
              " 'more',\n",
              " 'advanced',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " '(',\n",
              " 'NLP',\n",
              " ')',\n",
              " 'techniques',\n",
              " 'and',\n",
              " 'can',\n",
              " 'produce',\n",
              " 'more',\n",
              " 'concise',\n",
              " 'and',\n",
              " 'fluent',\n",
              " 'summaries',\n",
              " '.',\n",
              " '\\n',\n",
              " 'Examples',\n",
              " 'of',\n",
              " 'abstractive',\n",
              " 'summarization',\n",
              " 'techniques',\n",
              " 'include',\n",
              " ':',\n",
              " '\\n',\n",
              " 'T5',\n",
              " 'Transformers',\n",
              " ':',\n",
              " 'This',\n",
              " 'model',\n",
              " 'uses',\n",
              " 'a',\n",
              " 'transformer',\n",
              " 'architecture',\n",
              " 'to',\n",
              " 'generate',\n",
              " 'a',\n",
              " 'summary',\n",
              " 'text',\n",
              " '.',\n",
              " '\\n',\n",
              " 'BART',\n",
              " 'Transformers',\n",
              " ':',\n",
              " 'This',\n",
              " 'model',\n",
              " 'uses',\n",
              " 'a',\n",
              " 'bidirectional',\n",
              " 'encoder',\n",
              " 'representation',\n",
              " 'from',\n",
              " 'transformers',\n",
              " '(',\n",
              " 'BERT',\n",
              " ')',\n",
              " 'to',\n",
              " 'generate',\n",
              " 'a',\n",
              " 'summary',\n",
              " 'text',\n",
              " '.',\n",
              " '\\n',\n",
              " 'GPT-2',\n",
              " 'Transformers',\n",
              " ':',\n",
              " 'This',\n",
              " 'model',\n",
              " 'uses',\n",
              " 'a',\n",
              " 'transformer',\n",
              " 'architecture',\n",
              " 'to',\n",
              " 'generate',\n",
              " 'a',\n",
              " 'summary',\n",
              " 'text',\n",
              " '.',\n",
              " '\\n',\n",
              " 'XLM',\n",
              " 'Transformers',\n",
              " ':',\n",
              " 'This',\n",
              " 'model',\n",
              " 'uses',\n",
              " 'a',\n",
              " 'cross',\n",
              " '-',\n",
              " 'lingual',\n",
              " 'language',\n",
              " 'model',\n",
              " 'to',\n",
              " 'generate',\n",
              " 'a',\n",
              " 'summary',\n",
              " 'text',\n",
              " '.',\n",
              " '\\n',\n",
              " 'Implementation',\n",
              " 'in',\n",
              " 'Python',\n",
              " '\\n',\n",
              " 'Python',\n",
              " 'has',\n",
              " 'several',\n",
              " 'libraries',\n",
              " 'and',\n",
              " 'tools',\n",
              " 'that',\n",
              " 'can',\n",
              " 'be',\n",
              " 'used',\n",
              " 'for',\n",
              " 'text',\n",
              " 'summarization',\n",
              " ',',\n",
              " 'including',\n",
              " ':',\n",
              " '\\n',\n",
              " 'Gensim',\n",
              " ':',\n",
              " 'This',\n",
              " 'library',\n",
              " 'provides',\n",
              " 'a',\n",
              " 'range',\n",
              " 'of',\n",
              " 'methods',\n",
              " 'for',\n",
              " 'text',\n",
              " 'preprocessing',\n",
              " ',',\n",
              " 'topic',\n",
              " 'modeling',\n",
              " ',',\n",
              " 'and',\n",
              " 'document',\n",
              " 'similarity',\n",
              " 'analysis',\n",
              " '.',\n",
              " '\\n',\n",
              " 'Sumy',\n",
              " ':',\n",
              " 'This',\n",
              " 'library',\n",
              " 'provides',\n",
              " 'a',\n",
              " 'range',\n",
              " 'of',\n",
              " 'methods',\n",
              " 'for',\n",
              " 'text',\n",
              " 'summarization',\n",
              " ',',\n",
              " 'including',\n",
              " 'extractive',\n",
              " 'and',\n",
              " 'abstractive',\n",
              " 'techniques',\n",
              " '.',\n",
              " '\\n',\n",
              " 'NLTK',\n",
              " '(',\n",
              " 'Natural',\n",
              " 'Language',\n",
              " 'Toolkit',\n",
              " '):',\n",
              " 'This',\n",
              " 'library',\n",
              " 'provides',\n",
              " 'a',\n",
              " 'range',\n",
              " 'of',\n",
              " 'tools',\n",
              " 'and',\n",
              " 'resources',\n",
              " 'for',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " 'tasks',\n",
              " ',',\n",
              " 'including',\n",
              " 'text',\n",
              " 'summarization',\n",
              " '.',\n",
              " '\\n',\n",
              " 'Conclusion',\n",
              " '\\n',\n",
              " 'Text',\n",
              " 'summarization',\n",
              " 'is',\n",
              " 'a',\n",
              " 'critical',\n",
              " 'task',\n",
              " 'in',\n",
              " 'NLP',\n",
              " 'that',\n",
              " 'involves',\n",
              " 'condensing',\n",
              " 'a',\n",
              " 'large',\n",
              " 'amount',\n",
              " 'of',\n",
              " 'text',\n",
              " 'into',\n",
              " 'a',\n",
              " 'shorter',\n",
              " 'form',\n",
              " 'while',\n",
              " 'preserving',\n",
              " 'the',\n",
              " 'essential',\n",
              " 'information',\n",
              " '.',\n",
              " 'There',\n",
              " 'are',\n",
              " 'various',\n",
              " 'approaches',\n",
              " 'to',\n",
              " 'text',\n",
              " 'summarization',\n",
              " ',',\n",
              " 'including',\n",
              " 'extractive',\n",
              " 'and',\n",
              " 'abstractive',\n",
              " 'methods',\n",
              " '.',\n",
              " 'Python',\n",
              " 'has',\n",
              " 'several',\n",
              " 'libraries',\n",
              " 'and',\n",
              " 'tools',\n",
              " 'that',\n",
              " 'can',\n",
              " 'be',\n",
              " 'used',\n",
              " 'for',\n",
              " 'text',\n",
              " 'summarization',\n",
              " ',',\n",
              " 'including',\n",
              " 'Gensim',\n",
              " ',',\n",
              " 'Sumy',\n",
              " ',',\n",
              " 'and',\n",
              " 'NLTK',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# COUNTING FREQUENCY\n",
        "word_freq = {}\n",
        "for word in doc:\n",
        "  if word.text.lower() not in stopwords and word.text not in punctuation:\n",
        "    if word.text not in word_freq.keys():\n",
        "      word_freq[word.text] = 1\n",
        "    else:\n",
        "      word_freq[word.text] += 1\n",
        "print(word_freq)"
      ],
      "metadata": {
        "id": "aXUJ6GWX9cfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating maximum frequency"
      ],
      "metadata": {
        "id": "8IAo-VfBBI78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_freq = max(word_freq.values())\n",
        "max_freq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQ5qJLRb--pi",
        "outputId": "fdc77193-1392-4623-9d39-62ad6e55956c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Purpose of Normalization:\n",
        " Normalizing the word frequencies helps in various NLP tasks where different features need to be on a similar scale for comparison or further processing. For example, normalized frequencies are often used in text summarization, keyword extraction, or as input features for machine learning models"
      ],
      "metadata": {
        "id": "QyDLVShCBA_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating unit frequency\n",
        "# normalizing the frequency of words in a dictionary word_freq\n",
        "# by dividing each word's frequency by the maximum frequency value (max_freq)\n",
        "for word in word_freq.keys():\n",
        "  word_freq[word] = word_freq[word]/max_freq\n",
        "word_freq"
      ],
      "metadata": {
        "id": "r-G5Nt3W_oEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### list of sentence tokens"
      ],
      "metadata": {
        "id": "MUhuopTBBjAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokens = [sent for sent in doc.sents]"
      ],
      "metadata": {
        "id": "8skSSp1dANub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokens"
      ],
      "metadata": {
        "id": "AQrNX-IRBo-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### calcuating sentence score"
      ],
      "metadata": {
        "id": "-YeRPGLtEDak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent_scores = {}    # dictionary to store sentence scores\n",
        "for sent in sent_tokens:    # for each sentence\n",
        "  for word in sent:           # for each word in selected sentence\n",
        "    if word.text in word_freq.keys():       # if the selected word is in word_freq list or not\n",
        "      if sent not in sent_scores.keys():      # if the sentence is not in sent_scores list or not\n",
        "        sent_scores[sent] = word_freq[word.text]    # assign the word frequency to the sentence\n",
        "      else:\n",
        "        sent_scores[sent] += word_freq[word.text]    # add the word frequency to the sentence\n",
        "sent_scores"
      ],
      "metadata": {
        "id": "RdGu2rbvBt6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Summarization**\n",
        "\n"
      ],
      "metadata": {
        "id": "eMelPSkWE9h3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate one-third of the length of the list sent_tokens\n",
        "select_len = len(sent_tokens)//3\n",
        "select_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDMlyaBFB254",
        "outputId": "5a6a1044-f9dd-4f89-9d58-5f2dec5ca6ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from heapq import nlargest"
      ],
      "metadata": {
        "id": "KLHmVutzGUIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select top one third sentence based on sentence scores.\n",
        "summary = nlargest(select_len, sent_scores, key=sent_scores.get)\n",
        "# When used as key=sent_scores.get, it means that nlargest will retrieve\n",
        "# the largest elements based on the values returned by sent_scores.get(key).\n",
        "summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hVLs1bnCFiiz",
        "outputId": "37d64ecc-9647-4a0b-8fa2-cada14ef3d18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[text summarization techniques\n",
              " Text Summarization Techniques Overview\n",
              " Text summarization is a crucial task in natural language processing (NLP) that involves condensing a large amount of text into a shorter form while preserving the essential information.,\n",
              " Implementation in Python\n",
              " Python has several libraries and tools that can be used for text summarization, including:\n",
              " Gensim: This library provides a range of methods for text preprocessing, topic modeling, and document similarity analysis.,\n",
              " Examples of abstractive summarization techniques include:\n",
              " T5 Transformers: This model uses a transformer architecture to generate a summary text.,\n",
              " Extractive Summarization\n",
              " Extractive summarization involves selecting the most relevant sentences or phrases from the original text to form a summary.,\n",
              " Conclusion\n",
              " Text summarization is a critical task in NLP that involves condensing a large amount of text into a shorter form while preserving the essential information.,\n",
              " Abstractive Summarization\n",
              " Abstractive summarization involves generating a new summary text by paraphrasing the original content.,\n",
              " This library provides a range of methods for text summarization, including extractive and abstractive techniques.,\n",
              " This library provides a range of tools and resources for natural language processing tasks, including text summarization.]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print this in sting format\n",
        "final_summary = [word.text for word in summary]\n",
        "summary = ' '.join(final_summary)\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyDxBjnBGjko",
        "outputId": "75837086-b7a6-4f16-eb11-7b51960dee78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text summarization techniques\n",
            "Text Summarization Techniques Overview\n",
            "Text summarization is a crucial task in natural language processing (NLP) that involves condensing a large amount of text into a shorter form while preserving the essential information. Implementation in Python\n",
            "Python has several libraries and tools that can be used for text summarization, including:\n",
            "Gensim: This library provides a range of methods for text preprocessing, topic modeling, and document similarity analysis.\n",
            " Examples of abstractive summarization techniques include:\n",
            "T5 Transformers: This model uses a transformer architecture to generate a summary text.\n",
            " Extractive Summarization\n",
            "Extractive summarization involves selecting the most relevant sentences or phrases from the original text to form a summary. Conclusion\n",
            "Text summarization is a critical task in NLP that involves condensing a large amount of text into a shorter form while preserving the essential information. Abstractive Summarization\n",
            "Abstractive summarization involves generating a new summary text by paraphrasing the original content. This library provides a range of methods for text summarization, including extractive and abstractive techniques.\n",
            " This library provides a range of tools and resources for natural language processing tasks, including text summarization.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"number of workd in text: \", len(text.split(' ')))\n",
        "print(\"number of workd in summary: \", len(summary.split(' ')))"
      ],
      "metadata": {
        "id": "69qR0KLJJSv-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb4381a3-fe50-4157-a7bc-93c66f4ee823"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of workd in text:  365\n",
            "number of workd in summary:  173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uUPLC7JxP7dn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}